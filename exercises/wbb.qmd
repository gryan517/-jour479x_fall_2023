##Simulations
```{r}
library(tidyverse)
library(ggalt)
```
```{r}
set.seed(1234)

simulations <- rbinom(n = 1000, size = 20, prob = .317)

table(simulations)
```
So were saying, if you did this a thousand more times, we would not get these results. What were talking about is if we would expect to see this outcome. How likely or not is it to occur - how uncommon is it? 

Simulations are good way to check your impulse of going too over or under - hot or cold. 

```{r}
library(dplyr)

set.seed(1234)

simulations <- rbinom(n = 1000, size = 1, prob = .377)

four_in_a_row <- mean(simulations == 1 & lead(simulations, 1) == 1 & lead(simulations, 2) == 1 & lead(simulations, 3) == 1)

odds <- 1/four_in_a_row
```

In this case this player is so unlikely for going 4 games without scoring. 


##Beeswarm plots
```{r}
library(tidyverse)
library(ggbeeswarm)
library(ggrepel)
```

```{r}
set.seed(1234)
```

```{r}
players <- read_csv("https://thescoop.org/sports-data-files/wbb_players_2023.csv")
```
```{r}
activeplayers <- players |> filter(mp>0, position !="NULL") 
activeplayers <- activeplayers |> mutate(tspct=pts/(2*(fga+0.44*fta)))
summary(activeplayers$fga)
```
Above we are looking at number of FG. 3rd quartile would be considered the better shooters, the players that may be above average. 
```{r}
shooters <- activeplayers |> filter(fga > 187)
ggplot() + geom_beeswarm(data=shooters, aes(x=position, y=tspct), color="grey")
```
```{r}
umd <- activeplayers |> 
  filter(team == "Maryland") |> 
  filter(fga>187) |> 
  arrange(desc(tspct))
```


```{r}
ggplot() + 
  geom_beeswarm(
    data=shooters, 
    groupOnX=TRUE, 
    aes(x=position, y=tspct), color="grey") + 
  geom_beeswarm(
    data=umd, 
    groupOnX=TRUE, 
    aes(x=position, y=tspct), color="red") +
  geom_text_repel(
    data=umd, 
    aes(x=position, y=tspct, label=player))
```

```{r}
ggplot() + 
  geom_quasirandom(
    data=shooters, 
    groupOnX=TRUE, 
    aes(x=position, y=tspct), color="grey") + 
  geom_quasirandom(
    data=umd, 
    groupOnX=TRUE, 
    aes(x=position, y=tspct), color="red") +
  geom_text_repel(
    data=umd, 
    aes(x=position, y=tspct, label=player))
```

```{r}
ggplot() + 
  geom_jitter(
    data=shooters, 
    aes(x=position, y=tspct), color="grey") + 
  geom_jitter(
    data=umd, 
    aes(x=position, y=tspct), color="red") +
  geom_text_repel(
    data=umd, 
    aes(x=position, y=tspct, label=player))
```

##Encirclling points on a scatterplot
```{r}
players <- read_csv("https://raw.githubusercontent.com/dwillis/hhs-snapshots/main/data/player_totals_20231130.csv") |> filter(mp > 0)


topscorers <- players |> filter(pts > 175)

ggplot() + 
  geom_point(data=players, aes(x=mp, y=pts), color="grey") + 
  geom_point(data=topscorers, aes(x=mp, y=pts), color="black") + 
  geom_text(data=topscorers, aes(x=mp, y=pts, label=full_name), hjust = 0, vjust=1) +
  geom_encircle(data=topscorers, aes(x=mp, y=pts), s_shape=.5, expand=.03, colour="red") +
  geom_text(aes(x=275, y=275, label="Top scorers")) + 
  labs(title="Caitlin Clark Alone At Top", subtitle="The Iowa star is by far the top scorer among all NCAA players", x="Minutes", y="Points") + 
  theme_minimal() + 
  theme(
    plot.title = element_text(size = 16, face = "bold"),
    axis.title = element_text(size = 8), 
    plot.subtitle = element_text(size=10), 
    panel.grid.minor = element_blank()
    )
```
Including people who have at least played one minute. 

##Parity
```{r}
logs <- read_csv("https://thescoop.org/sports-data-files/wbblogs24.csv")
rankings <- read_csv("https://thescoop.org/sports-data-files/wbb_rankings.csv")
logs23 <- read_csv("https://thescoop.org/sports-data-files/wbblogs23.csv")
```
#Parity Index = (SRS + 100) / (SOS + 100)
To deal with positive values. 
```{r}
parity_index24 <- logs |> 
  group_by(Team, Conference) |> 
  summarise(srs_score = mean(TeamSRS), sos_score = mean(TeamSOS)) |> 
  mutate(parity_index = (srs_score + 100) / (sos_score + 100))

parity_index23 <- logs23 |> 
  group_by(Team, Conference) |> 
  summarise(srs_score = mean(TeamSRS), sos_score = mean(TeamSOS)) |> 
  mutate(parity_index = (srs_score + 100) / (sos_score + 100))
```
The higher the parity score the better they should perform. SRS is high you're winning a lot. 

```{r}
parity_index24 |> 
  filter(Conference == 'Big Ten WBB') |> 
  ggplot() + 
  geom_point(aes(x=srs_score, y = sos_score, label = Team)) +
  geom_text(aes(x=srs_score, y = sos_score, label = Team))
```
Big Ten seems to be more internally competitive. 

```{r}
parity_with_top25 <- parity_index24 |> left_join(rankings, join_by(Team == team))
```

The higher the parity is the higher they are likely to compete against more difficult opponents, not they are necessarily better.

```{r}
combined_data <- bind_rows(parity_index24 |> mutate(season="2024"), parity_index23 |> mutate(season = "2023")) 

result <- combined_data %>%
  group_by(Team) %>%
  summarize(
    Parity_Index_2024 = mean(parity_index[season == "2024"]),
    Parity_Index_2023 = mean(parity_index[season == "2023"])
  ) %>%
  filter(!is.na(Parity_Index_2024)) |> 
  filter(!is.na(Parity_Index_2023)) |>
  ungroup() %>%
  summarise(
    p_value = t.test(Parity_Index_2024, Parity_Index_2023, paired = TRUE)$p.value,
    mean_difference = mean(Parity_Index_2024 - Parity_Index_2023)
  )
```

p-value over .05 and close to 1 there's little to distinguish in the means of parity indexes - it could be a result of random chance. Mean difference is also very tiny. 


The trouble in comparing last season and a portion of the current season can not be helpful in that your comparing a entire season to part of a season or helpful in seeing that playing cupcake teams early on should have had a difference in last season (i.g. lower p-value, higher mean).
